---
title: "Prediction Assignment Writeup"
author: "Mike"
date: "November 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is my assignment on Practical Machine Learning by Coursera (John Hopkins University). 

## Background and Objective
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

## Loading required packages

```{r, echo=FALSE}

library(rpart.plot)
library(rpart)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(plyr)
library(caret)
library(knitr)
library(caret)
library(rattle)
library(corrplot)

```
All necessary packages should already loaded and the files should  downloaded. We should clean the missing data (NA) and missing values in training data sets. Those values are useless and can be disregarded 
```{r, echo=FALSE}
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "./pml-training.csv")
dtraining <- read.csv("./pml-training.csv", na.strings=c("NA","#DIV/0!",""))
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "./pml-testing.csv")
dtesting <- read.csv("./pml-testing.csv", na.strings=c("NA","#DIV/0!",""))

```

## Partition of Data
We will now partition our data into a training data set and a testing data set (60% and 40% of the total cases respectively)

```{r, echo=FALSE}
valid <- names(dtesting[,colSums(is.na(dtesting)) == 0])[8:59]
dtraining <- dtraining[,c(valid,"classe")]
dtesting <- dtesting[,c(valid,"problem_id")]
dim(dtraining); dim(dtesting);
```

```{r}
set.seed(12345)

Train <- createDataPartition(dtraining$classe, p=0.6, list=FALSE)
training <- dtraining[Train,]
testing <- dtraining[-Train,]

dim(training); dim(testing);
```

## Prediction MOdelling
  We will now building a Random Forest Model and  Decison Tree
  
## Decision Tree

```{r}
set.seed(301)
modFitDecTree <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(modFitDecTree)
```
```{r, echo=FALSE}
predictTree <- predict(modFitDecTree, newdata=testing, type="class")
confTree <- confusionMatrix(predictTree, testing$classe)
confTree

plot(confTree$table, col = confTree$byClass, 
     main = paste("Decision Tree - Accuracy =",
                  round(confTree$overall['Accuracy'], 4)))
```

## Building a Random Forest Model
The out of sample error should be small when we use the Random Forest Model. The error will be estimated using the 40% testing sample.

```{r}
set.seed(12345)

modFitRF <- randomForest(classe ~ ., data = training, method = "rf", importance = T, trControl = trainControl(method = "cv", classProbs=TRUE,savePredictions=TRUE,allowParallel=TRUE, number = 10))

plot(modFitRF)
```

## Predicting with Random Forest Model
```{r}
prediction <- predict(modFitRF, testing, type = "class")
confusionMatrix(prediction, testing$classe)
```
The random forest model has 99.3% Accuracy.

## Choosing the Model in Predicting to test the data
We will now applying the random forest model with an accuracy of 99.3% over Decision tree.

```{r}
predictTEST <- predict(modFitRF, newdata=testing)
predictTEST
```

